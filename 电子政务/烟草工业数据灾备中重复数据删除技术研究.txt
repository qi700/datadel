烟草工业数据灾备中重复数据删除技术研究
张志杰 何利力 来源：万方数据关键字：重复数据删除 烟草工业 容灾备份信息化调查找茬投稿收藏评论好文推荐打印社区分享首先介绍了烟草工业灾备中重复数据删除技术重要要性，然后介绍了重复数据删除的最主要两种方法：基于哈希算法的重复数据删除和基于内容识别的重复数据删除。最后重点分析设计了适用于某综合营销平台系统数据备份恢复的重复数据删除策略，以减少网络带宽占用、节省存储空间、保护业务数据，为综合营销和智能决策的发展提供强有力的支撑。    1.工业数据灾备　　灾难性事故所导致的信息系统崩溃的事件时有发生。在烟草工业，随着烟草一体化建设的推进，数据中心的范畴已逐步形成，数据和系统也逐步集中。结合小型机的双机容错技术。数据中心顺利降低了单点故障的风险，为业务提供持续和有效的服务能力。然而，随着系统和数据的集中，所有数据存储在一个机房，如果灾难性事故导致机房受到破坏，其内的所有数据包括备份数据也会随之破坏，其带来的风险和损失是无法估量的。而数据异地备份能有效解决这个问题。随之而来的问题是要降低数据的存储容量和降低数据网络传输的开销以及加快备份速度。烟草工业数据主要存在以下两个特点：　　1)每天新增数据量大。比如：仅仅在销售环节，全国零售户有大概500万家，平均每周进化一次，假设平均每家只销售15个规格，那么平均每天产生销售记录数为：500x15+7=1070(万)。　　2)数据变动大、重复率高。比如，在笔者所参与的综合营销平台中，仅办公自动化子系统，每天文件流转、版本修订、邮件附件抄送都非常普遍。有时候一个文件要抄送给多人，有时候同一原始文件会产生多个修订版本，使得文件系统中保存有大量的重复数据。　　重复数据删除技术是一种能够大规模消除冗余数据，降低数据存储成本的重要技术。它的工作方式是查找不同文件中不同位置的重复数据块，重复的数据块用指示符取代，使得在备份中重复数据块只保留一份，从而可以在已有的磁盘上存储更多的备份数据。重复数据删除后，再通过WAN进行异地备份，则能减少备份数据量，节省网络带宽，加快数据备份和恢复速度。本文将围绕企业数据灾备，研究如何使用重复数据删除技术，提高数据存取效率，降低数据保护成本。　　2.重复数据删除方法　　2.1 基于哈希(Hash)算法的重复数据删除　　哈希算法主要用于文件级和数据块级别的重复数据删除。在重复数据删除技术的实现中，通常采用SHA-1和MD5算法计算并检查数据块的“指纹”，判断该数据块是否与已经存在数据块重复。如果该数据块已经存在，则只需要保留指向该数据块的指针，否则，则要保留该数据块，并将该数据块的“指纹”保存在索引表中，供以后使用。基于哈希算法的重复数据删除的流程图如图1所示：图1 基于哈希算法的重复数据删除流程　　文件级的重复数据删除主要是识别内容完相同的两个文件，从而避免相同文件多个备份。用散列函数计算文件哈希值的方法来比较文件是否相同，可以快速的扫描整个目录，查找速度非常快。文件级的重复数据删除的缺点也很明显，就是当文件稍稍修改了一点，都会变成不同的文件，重复数据删除率会大打折扣。　　数据块级的重复数据肭除就是将文件分块，然后进行重复删除。分块的方式有固定大小分块和可变大小分块。两者相比，固定分块可以更快的扫描新到数据流，获取更高重复删除速率；而可变分块可以提供更大的重复数据删除率。两者之间根本差异即空间和时间的矛盾，实际应用中，还要根据应用环境和需求来选择相应分块方式。　　通常，为了快速识别数据块是否已经备份，会将哈希索引保留在内存中。当备份的数据块数据增加时，索引也增加。因此，总有一天，索引会将内存填满。现在大部分基于散列的系统的都是独立的。　　2.2 基于内容识别的重复数据删除　　从字节级别上分析数据流通常能够“识别内容”。这种方法主要是对比记录的数据格式。在备份数据时，首先从数据流中提取元数据，并将之与备份系统中已经存储的元数据进行对比。当元数据匹配成功时，则将新的数据对象与备份系统中对应的数据对象进行逐字节比较，如果完全相同，则删除新数据，用备份系统中的数据对象索引替换；如果不同，则找出发生变化的数据，将增量保存并计算并插入索引。该方法的流程如图2所示：图2 基于内容识别的重复数据删除流程图　　3.综合营销平台备份策略分析设计　　笔者参与开发的综合营销平台主要功能有：OA办公、业务处理、决策支持和会员俱乐部等。平台数据存储备份整体架构如图3所示：图3 平台数据存储备份整体架构　　由此平台系统的功能决定，工作人员主要是在白天(上班时间)使用系统，系统产生大量企业内业务数据和办公数据；而决策支持的所需大批量数据主要由国家局每天下行导入本系统。为了保证业务的正常进行，白天需要把计算机的CPU资源和内存资源尽量用给业务功能，因此，本系统对国家下行数据的抽取、整理和重复数据删除只能在夜里进行。首先，由于决策分析的需要，需要在每天夜里把下行的数据(数据量非常大，大概有7G到10G)进行完全加工，而这需要较长的时间(通过升级硬件资源和优化算法可能将时间稍稍减少)；其次本地存储系统和异地存储之间网络带宽的限制，在每天上班前要把每天的数据备份完毕也需要较长的时间。因此，重复数据删除的策略如下：　　1)首先对于本地存储的数据，对每天新增的数据进行重复数据删除，可以减少数据远程备份的网络流量，减少带宽的占用。为能够较快的完成重复数据删除，采用基于哈希算法固定分块的重复数据删除策略，在分块散列查找时采用一种滑动窗1：3的方式，期望尽可能的发现重复数据块。　　2)对于异地存储，由于业务的需要对数据的真实性要求非常高，而现有的在线处理方式是在数据存入设备的同时时行重复数据删除，并没有进行严格的校验和核对。万一数据处理的环节发生一个小错误。可能导致整个备份变成无法使用的东西，给企业带来损失。而且异地设备除了进行数据存储，一般不需要进行其它工作。因此采用后处理重复数据删除方式。并采用基于基于哈希算法变长分块的重复删除数据策略，尽可能的发现重复数据，提高存储的使用率，在不增加存储的情况下备份更多的数据。　　3)因为哈希算法存在哈希冲突的问题，此系统采用一种优化了的方法，对哈希匹配的数据对象，进行二进制的比对，若完全一致，才能进行重复数据删除。　　4)为了能以较快、较高效率的进行重复数据删除，在本地和异地执行重复数据删除前，先利用系统的智能引擎模块探测新增数据的文件名、文件各类和日期/时间戳等信息，然后进行有规律的分块，再通过数据压缩技术进行压缩。这之后再进行重复数据删除。采用这种策略可以极大的提高执行重复数据删除的效率。　　5)对于非常重要的业务数据和要求能够迅速恢复的数据(比如职员信息，最近的业务订单等)，则不进行重复数据删除，以便在发生故障时能够迅速恢复，不影响工作的开展。　　4.结束语　　配置管理自产生至今经过几十年的发展，理论体系日臻成熟，是一种系统、高效的质量管理体系。在技术上。配置管理能够建立对于核电数字化仪控系统原始设计和中间变更的控制和审查、批准流程，为核电项目的安全性和可靠性奠定基础。在经济上，配置管理保证配置项描述文档及时的反应已经实施的变更，从而有效的控制变更，控制项目成本。在管理上，它为项目管理提供了各种监控项目进展的视角，为项目经理确切掌握项目进程提供了保证。