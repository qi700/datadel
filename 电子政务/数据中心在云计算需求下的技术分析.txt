数据中心在云计算需求下的技术分析
对当前云计算环境下为互联网数据中心（IDC）带来的新特性及需求进行分析，针对现有非云计算数据中心的网络体系、协议和管理总结其在云计算场景下的不足。同时归纳目前阶段云计算数据中心领域在新需求下出现的主要新技术，并对未来的研究思路及待解决问题进行展望。引言    在云计算的环境下，计算资源、网络资源和存储资源都可以作为业务发放给用户，从而使得资源共享变得更加灵活和广泛，减少了用户的硬件购置，升级维护等成本终端用户可以使用各种类型的瘦客户端，在任何地方接入建立在云计算上的各类应用。理论上来说，相对于应用装在用户本地终端的传统方法，云计算用户应该获得相同或甚至更好的用户体验。为了达到这样的效果，对于支撑云计算的数据中心就有了更多新的要求。本文将逐一阐述这些数据中心的新特性，并探讨为了满足这些新特性而产生的核心技术及其演进预期。本文所述的数据中心都是指互联网数据中心fIn—ternet Data Center，IDC)类型的数据中心。1 云计算为数据中心带来的新特性    在云计算场景下，IDC的运营商为各类企业提供各种服务，包括云计算的应用、网络和存储服务。    1.1服务器虚拟化和自由迁移    在传统的非云计算场景下，物理服务器通常只被单一租户的单一业务使用，所以单台服务器的利用率会比较低。随着业务的扩展，需要不断增加服务器的数量，这样会使得数据中心的规模越来越庞大。    在云计算的场景下，由于IDC为大量而不是单一的企业或用户提供服务，所以业务量会极其庞大。靠不停地增加物理服务器对于成本和网络规模来说都难以接受。虚拟化技术使得一台物理服务器可以被虚拟成多台服务器来使用，从而利用了原本闲置的资源，提高了服务器的使用率，所以使用相对较少的物理服务器就能满足IDC的业务需要。通常对于多核CPU的服务器来说，可以虚拟成每个CPU一个虚拟机来使用。对于某些CPU资源消耗不太大的应用，例如VDI(虚拟桌面)，甚至一个CPU可以虚拟成四个或更多的虚拟机。当然虚拟化还要受到硬盘的每秒I/O数和内存的限制。    除了提高使用率外，虚拟化还使得服务器自由迁移变成可能。在传统的数据中心，进行服务器的迁移是一项非常浩大的工程。必须事先进行规划，需要谨慎计划割接时间，做好备份。服务器需要进行断线断电，搬移，重新上电上线，通常业务会中断，所以搬迁服务器是极少发生的。而使用了虚拟化技术以后，虚拟机的迁移不再涉及到物理上的搬迁。并且可以使用各种技术，例如渐进式内存复制等方法使得迁移平滑进行，保证了迁移时用户不感知，相关业务不中断，不受影响。自由迁移为数据中心的容灾备份，节能环保，网规网优提供了不可替代的便利。虚拟机的自由迁移满足了云计算环境下用户随时随地进行数据接入的需求。    1.2支持多租户    在非云计算的时代，通常各个租户或者企业都需要自建数据中心或者租用运营商的硬件和基础设施用于自己的服务计算和数据存储。在这种情况下，可以理解为租户自己拥有独立的数据中心，可以自己进行运维也可以将运维托管给运营商。在使用了云计算以后，企业或者租户可以向提供云计算的运营商租用计算资源，网络资源，存储资源，而不再是租用硬件和基础设施，从而简化或完全不需自建及自行运维数据中心。另外，云计算运营商的数据中心则会变得更加的复杂和庞大。其中的一个挑战就是支持多租户。    支持多租户主要体现在:    (1)针对每个租户业务的快速配置和部署。每个租户在云计算运营商申购的资源需要能够快速自动地在数据中心的网络中使能，还包括防火墙、IPS/IDS , LoadBalancer等设备的相应配置，做到即插即用。    (2)租户之间的流量隔离。实际上，由于租户之间可能共享硬件设备、带宽、存储等资源，为了保证安全，在以二层网络为主的数据中心，租户之间的流量需要进行隔离，防止租户甲的流量被租户乙接收到。传统的二层网络基于VLAN(虚拟局域网)进行流量隔离的方法受限于4096个的VLAN数量限制。    (3)网络配置和与租户应用相关的配置解藕。    1.3无阻塞网络    支持云计算的数据中心吞吐量极大的增加。由于云计算数据中心内部资源节点(服务器、存储、数据库等)之间的访问需求和交互流量相比于传统数据中心都将大大增加，所以云计算数据中心吞吐量会对组网技术提出了严峻挑战。另外，云计算数据中心经常采用分布式计算。分布式计算对数据中心的流量模型带来了巨大的变化，流量模型从以纵向流量(南北向流量，用户访问服务器流量)为主转变为以横向流量(东西向流量，服务器之间的流量)为主。    传统的数据中心的网络架构多是基于树形的，下行和上行的带宽有收敛比。这样就会造成拥塞。在云计算数据中心，需要构建无阻塞的网络来满足流量需求。基于胖树结构的拓扑将被用于构建无阻塞的网络。胖树解决了树结构根节点易成为通信瓶颈的问题。胖树结构中，越接近根节点，链路的带宽越大。对于理想的胖树结构，上层链路带宽应为下层链路带宽之和。图1显示了胖树的结构。  图1 胖树的结构示意图使用胖树结构的数据中心网络拓扑，要求每台边缘交换机和所有核心交换机都建立连接，同时，核心交换机和每台边缘交换机之间有且仅有一条链路连接。从边缘交换机到核心交换机之间的流量均衡是降低网络拥塞的关键。图2显示了基于胖树结构的网络物理拓扑图。  图2 基于胖树的网络物理拓扑2 云计算数据中心的网络新技术    为了满足前面所述的新需求，一些新的适用于云计算数据中心的网络技术也随之出现。这些新技术分别着重解决一个或多个问题。作为云计算数据中心的整体解决方案，很多技术需要联合使用。这些新技术有些还处于研究或者优化的阶段，随着业界对于云计算数据中心理解的加深以及商业模式或应用的拓展，可以预见更多的技术或者优化将会被提出及使用。    2.1虚拟感知    当越来越多的服务器可以支持虚拟化以后，接入层的概念不再仅针对物理端口，而是延伸到服务器内部，为不同虚拟机之间的流量交换提供服务，将虚拟机同网络端口关联起来。虚拟机之间的数据交换通常由虚拟交换机来完成，但内置在虚拟化平台上的软件虚拟交换机(VSwitch)无法被网络设备感知，甚至也不由网络管理员管理。所以这种使用网络不感知的虚拟交换机的方法无法实现虚拟机之间通信的流量监管、虚拟交换机端口策略等功能。为了解决上述问题，如图3所示，IEEE 802，1Qbg Edge Virtual Bridging (EVB)定义了关于虚拟机网络接入的技术标准。  图3 EVB(边缘虚拟桥接)架构图    如图3所示，支持一或多个虚拟机附着的端站称为EVB，每个虚拟机有至少一个虚拟机接口VSI，每个虚拟机可以和其他虚拟机通信，或者通过边界中继ER和桥接LAN上的其他虚拟机通信。传统的虚拟交换机在内部对于同一个物理服务器的不同VM直接可以直接进行流量转发，这种方式被成为VEB (Virtual EdgeBridging，虚拟边缘桥接)。除了VEB之外，一种新的方式VEPA(Virtual Ethernet Port Aggregator，虚拟以太端口汇聚)也被制订。VEPA的核心思想是，将虚拟机产生的网络流量全部交由与服务器相连的物理交换机进行处理，即使同一台服务器的虚拟机间流量，也发往外部物理交换机进行转发处理。    VEPA的目标是要将虚拟机之间的交换行为从服务器内部移出到上联交换机上，当两个处于同一服务器内的虚拟机要交换数据时，从虚拟机A出来的数据帧首先会经过服务器网卡送往上联交换机，上联交换机通过查看帧头中带的MAC地址(虚拟机MAC地址)发现目的主机在同一台物理服务器中，因此又将这个帧送回原服务器，完成寻址转发。整个数据流经历了一次回环((hairpin)，而这在传统的交换设备上是不被允许的。    EVB标准还定义了“多通道技术(Multichannel)”,可以使得VEB和VEPA同时存在于一台物理服务器，一个VEB或VEPA可以对应一个通道，这样为数据识别和流量管理，以及网络配置都提供了便利。    为了支持上述功能的实现，相应的虚拟感知和发现协议也同时被定义。当一个虚拟机上线的时候，它需要通过VDP(虚拟机发现协议)来和相邻的物理交换机进行通告和配置信息交换。这样临近的物理交换机上可以感知到虚拟机的上线，并对于网络策略或参数进行下发和配置。    除了IEEE 802.1Qbg以外，还有其他的一些机制和协议也用于虚拟感知和发现，例如IEEE802.1BR所描述的端口扩展技术。可以把虚拟端口统一映射到控制桥上，进行更集中的管理、配置和维护。    2.2租户Overlay网络    为了使得云计算数据中心可以支持大量的租户，租户Overlay网络的概念被提了出来。不同的租户之间流量隔离，且租户的数量远远大于传统12比特VLANID能提供的40%的租户数。在IP层之上提供一层overlay网络，使用特殊格式的封装即特殊的Overlay的报头来区分不同的租户。封装格式可以是多种多样的，例如VXLAN或者NVGRE的方式。无论Overlay的封装格式是怎样，它的核心思想都是映射加封装。    如图4所示，某租户的VM1发送报文给同一租户的VM2的时候，边缘入口节点S1将来自于租户源VM1的报文目的地址(即VM2的地址)映射为传送报文的隧道的出口节点地址(即S2的地址)。人口节点S1将原始报文进行封装通过隧道传递后，出口节点S2将封装剥除，恢复出原始报文并传递给最终的目的VM2o这个流程里面主要涉及两个问题，一是封装格式，二是控制平面对于映射的管理。封装格式前面已说过，可以是多种多样。而控制平面的对于映射和隧道的管理，则需要有比较统一的方法。理论上来说，可以分为两类，自发学习或通过控制信令。    自发学习的方法适用于比较小型的数据中心，类似于传统的2层地址学习。节点对接收到的报文进行内外层地址的对应关系学习，例如图4中S2上学习到(VM1, S1)的地址映射关系。自发学习的方法比较简单，但是通过控制信令的方式扩展性更好，更适用于大型的数据中心。隧道的端点需要通过控制信令来对注册和解注册相应的地址对应关系。VM在上线的时候它的地址和隧道的映射关系将被注册，在迁移的时候注册关系将被更新，在下线的时候注册关系将被删除。    租户Overlay的方法可以使得网络本身对于租户内部的地址管理透明，策略可以根据租户来下发。租户的流量隔离将会在边缘节点基于租户ID来实现，突破了传统的使用二层以太网VLAN来隔离的租户数目限制。 图4 租户Overlay网络    2.3大二层网络协议    传统的STP(生成树协议)用于防止二层网络的环路，但是在云计算数据中心中，STP有诸多的缺陷，主要包括:    (1)为了防止环路而对特定端口进行阻塞，导致链路利用率低;    (2)无法实现等价多路径转发;    (3)网络收敛速度慢。    在以胖树结构为主的云计算数据中心(如图2所示)，基于多条等价路径的转发是均衡流量防止拥塞的基础。另外，由于虚拟机迁移的需求，一般要求虚拟机在迁移后的IP地址不发生改变，也就是说虚拟机在同一个二层网络内进行迁移将会大大简化迁移对于三层网络的影响。在这种情况下，支持大规模二层网络的协议开始取代传统的STP协议。目前最常见的用于大二层的协议是TRILL (Transparent Interconnection of Lotsof Links，多链路透明互联)和SPB(Shortest Path Bridg-ing，最短路径桥接)。两者都是将三层的路由的思想引入到二层的数据转发的协议，融合了二、三层现有技术的优点而规避其缺点，采用现有的IS-IS路由协议计算和维护网络拓扑。    TRILL协议由IETF TRILL工作组制订。它引入了新的标识—别名(Nickname)用于数据转发的标识。运行TRILL协议的设备称为路由网桥，路由网桥之间运行IS-IS链路状态路由协议，由IS-IS链路状态路由协议计算网络拓扑，根据SPF算法计算用于单播数据转发的最短路径。对于未知单播，组播和广播报文，会根据分发树(Distribution Trees)进行报文转发。分发树是一种共享树，可以根据VLAN来进行剪枝。全网可以定义多棵分发树用于组播流量的负载均衡。为了防止环路，TRILL报文头携带了Hop Count字段，在路由网桥转发报文时，每经过一跳，该字段的值会减1。当该字段的值为0时，接收到该报文的路由网桥会丢弃该报文。对于组播转发，除了Hop Count字段还，路由网桥还会进行反向路径检测(Reverse Path Check)，以防止报文转发环路。    SPB协议由IEEE 802.1制订。它使用一个Common and Internal Spanning Tree(LIST)，支持每个Region自动决策，选择最大可能延伸连接所有的网桥和LAN,SPB连接的是网桥和网桥，主要作用在数据中心内部和客户端之间。SPB网络中仍采用传统Ethernet进行转发，因此定义了一系列的软件算法以保证多路径的广播无环和单播负载均衡，其特性主要包括:    (1)定义I-SID用于区分多个拓扑，I-SID信息在数据报文中以BVID(外层Ethernet报头中的VLAN Tag)形式携带，这样可以解决不同业务多拓扑转发的问题;    (2)到达所有相关UNI节点的SPT(Shortest PathTree)用于单播与组播报文的转发;    (3)ECT(Equal Cost Tree)以处理两个UNI间存在多条等价路径时负载均衡转发;    (4)使用以自己为根的多播树MT ( Multicast Tree )用于未知单播与广播报文转发。    任意两点间的Shortest Path一定是对称的;ECT的负载均衡是基于不同I-SID分担的。    在业界还有一些非标准化的协议，可以理解为TRILL或者SPB的变种，它们的基本思想都是一致的，在二层引入了三层的路由思想。    在不对三层网络进行改变的情况下，二层网络的规模越大，那么一个虚拟机可以迁移的范围越大。使用大二层协议可以充分利用胖树的结构优势实现流量无阻塞，并达到网络故障亚秒级收敛。3 结语    云计算数据中心在多个方面对于网络架构、协议管理、运维都带来了新的需求。目前来看，主要需求的来源可以归纳为虚拟化、大容量、多租户。由于数据中心的规模，支持的业务以及流量模型的不同，难以对于数据中心都采用完全统一的拓扑结构。但是无论数据中心是采用大二层架构还是三层延伸至架顶交换机的结构或是别的结构，都需要新技术的支持才能获得更好的性能。    目前多个标准组织，包括IETF,IEEE,DMTF等都在制订和云计算数据中心相关的标准。其中，IETF主要侧重三层和以上网络，以及网络业务的自动下发和网络资源的调度，IEEE主要侧重二层网络以及为了支持FCoE而设计的无丢包以太，DMTF更侧重应用层的配置管理和接口制定。另外还有相关的安全、存储等技术也在演进中。    包括第二节所述的一些技术都还处于不完全成熟的状态，另外在组播效率和规模支持、数据中心跨地域互联互通、存储和数据网络合一、集群、链路捆绑等方面需要进一步的技术优化。